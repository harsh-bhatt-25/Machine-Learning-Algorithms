{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a dataset according to the given scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Here we are generating dataset according to the scheme specified in question 1a.\n",
    "\n",
    "class GenerateDataset:\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.weights, self.y_array, self.x_array = [], [], []\n",
    "        \n",
    "        # Generate Weights\n",
    "        weight_sum = 0\n",
    "        for i in range(2, k + 1):\n",
    "            weight_sum += 0.9 ** i\n",
    "\n",
    "        for i in range(2, k + 1):\n",
    "            wi = (0.9 ** i) / weight_sum\n",
    "            self.weights.append(wi)\n",
    "\n",
    "    def generate_y_values(self, x_values, k):\n",
    "        # According to the x values generated, we are generating the labels for a particular datapoint.\n",
    "        check_sum = 0\n",
    "        y = -1\n",
    "        for i in range(1, k):\n",
    "            check_sum += self.weights[i - 1] * x_values[i]\n",
    "        if check_sum >= 0.5:\n",
    "            y = x_values[0]\n",
    "        else:\n",
    "            y = 1 - x_values[0]\n",
    "        return y\n",
    "\n",
    "    def generate_data_points(self, k):\n",
    "        # Generate x values for a row and subsequently generate the labels calling the above function.\n",
    "        x_values = [choices([0, 1], [0.5, 0.5])[0]]\n",
    "\n",
    "        for i in range(1, k):\n",
    "            x_values.append(choices([x_values[i - 1], (1 - x_values[i - 1])], [0.75, 0.25])[0])\n",
    "        y_value = self.generate_y_values(x_values, k)\n",
    "        self.y_array.append(y_value)\n",
    "        self.x_array.append(x_values)\n",
    "\n",
    "    def generate(self, m, k):\n",
    "        # Here we get the m and k values through which the dataset is generated.\n",
    "        for i in range(m):\n",
    "            self.generate_data_points(k)\n",
    "        return np.array(self.x_array), np.array(self.y_array)\n",
    "    \n",
    "# An example of dataset generated with 4 features and 10 datapoints.\n",
    "obj = GenerateDataset(4)\n",
    "x, y = obj.generate(10, 4)\n",
    "print(x) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Decision Tree using ID3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, value=None, *, left=None, right=None, label_name=None, entropy=None, samples=None,\n",
    "                 counts=None):\n",
    "        # Initializing the node class with a variety of values which helps us in representing numerous values easily. \n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label_name = label_name\n",
    "        self.entropy = entropy\n",
    "        self.samples = samples\n",
    "        self.counts = counts\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    flag = \"a\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "    \n",
    "    def train(self, x_array, y):\n",
    "        _, features = x_array.shape\n",
    "        self.root = self.tree(x_array, y)\n",
    "    \n",
    "    # Tree will grow using the below function.\n",
    "    def tree(self, x_array, y):\n",
    "        left_indices, right_indices = [], []\n",
    "        len_labels = len(set(y))\n",
    "        \n",
    "        # Checking if only single valued labels are present,  \n",
    "        if len_labels == 1:\n",
    "            label_name = y[0]\n",
    "            return Node(label_name=label_name)\n",
    "        \n",
    "        # Finding the column and value to split on using another function.\n",
    "        feature_index, entropy, feature_value = self.get_split_index(x_array, y, np.unique(y))\n",
    "        if entropy == 0:\n",
    "            value = self.max_counter(y)\n",
    "            return Node(label_name=value)\n",
    "        \n",
    "        # Splitting the array based on the values we got from the splitting function.\n",
    "        x_column = x_array[:, feature_index]\n",
    "        len_x_column = len(x_column)\n",
    "        for i in range(len_x_column):\n",
    "            if x_column[i] == feature_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "                \n",
    "        # Creating left and right branches of the current node.\n",
    "        left_node = self.tree(x_array[np.array(left_indices), :], y[np.array(left_indices)])\n",
    "        right_node = self.tree(x_array[np.array(right_indices), :], y[np.array(right_indices)])\n",
    "        return Node(feature_index, feature_value, left=left_node, right=right_node, entropy=entropy,\n",
    "                    samples=len(y), counts=np.bincount(y))\n",
    "    \n",
    "    # Counter function to split based on the counts of y values.\n",
    "    def max_counter(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length = len(counts)\n",
    "        max_count, max_value = -1, -1\n",
    "        for i in range(length):\n",
    "            if counts[i] > max_count:\n",
    "                max_value = i\n",
    "        return max_value\n",
    "    \n",
    "    # Looping over the indexes and value to find the one with highestn entropy.\n",
    "    def get_split_index(self, x_array, y, unique_y):\n",
    "        entropy, split_index, split_value = -1, None, None\n",
    "        for column in range(x_array.shape[1]):\n",
    "            X_column = x_array[:, column]\n",
    "            for value in unique_y:\n",
    "                gain = self.information_gain(X_column, y, value)\n",
    "                if gain > entropy:\n",
    "                    entropy = gain\n",
    "                    split_index = column\n",
    "                    split_value = value\n",
    "\n",
    "        return split_index, entropy, split_value\n",
    "    \n",
    "    # Finding the IG for a given column and value.\n",
    "    def information_gain(self, X_column, y, split_value):\n",
    "        H_Y = self.H(y)\n",
    "\n",
    "        left_indices, right_indices = [], []\n",
    "        length = len(X_column)\n",
    "        for i in range(length):\n",
    "            if X_column[i] == split_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return 0\n",
    "\n",
    "        y_length = len(y)\n",
    "        H_Y_X = 0\n",
    "        x_values = {len(left_indices): y[left_indices], len(right_indices): y[right_indices]}\n",
    "        for value in x_values:\n",
    "            x_probability = value / y_length\n",
    "            H_Y_X += x_probability * self.H(x_values[value])\n",
    "\n",
    "        IG = H_Y - H_Y_X\n",
    "        return IG\n",
    "\n",
    "    # Calculating the entropy using the formula taught.\n",
    "    def H(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length_y = len(y)\n",
    "        probabilities = np.array([i / length_y for i in counts])\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    \n",
    "    # This function will be called when we want to predict. Requires us to pass the datapoints as the parameter.\n",
    "    def predict(self, x_array):\n",
    "        return np.array([self.tree_traverse(x, self.root) for x in x_array])\n",
    "    \n",
    "    # Traversing the tree for the predict function.\n",
    "    def tree_traverse(self, x, node):\n",
    "        if node.label_name is not None:\n",
    "            return node.label_name\n",
    "\n",
    "        if x[node.feature] == node.value:\n",
    "            return self.tree_traverse(x, node.left)\n",
    "        return self.tree_traverse(x, node.right)\n",
    "    \n",
    "    # Using BFS to print the decision tree level by level with details on which it was created.\n",
    "    def print_tree(self, root):\n",
    "        if root.feature is None and root.label_name is None:\n",
    "            return\n",
    "        \n",
    "        print(\"Level 0 (Root Node) \")\n",
    "        queue = [root, self.flag]\n",
    "\n",
    "        while True:\n",
    "            current = queue.pop(0)\n",
    "            if current != \"a\":\n",
    "                if current.label_name is not None:\n",
    "                    print(\"Value = {}, Entropy = {}\".format(current.label_name, current.entropy, end=\" \"))\n",
    "                else:\n",
    "                    print(\"Feature = {}, Samples = {}, Counts = {}, Entropy = {}\".format(current.feature,\n",
    "                                                                                         current.samples,\n",
    "                                                                                         current.counts,\n",
    "                                                                                         current.entropy, end=\" \"))\n",
    "                if current.left is not None:\n",
    "                    queue.append(current.left)\n",
    "                if current.right is not None:\n",
    "                    queue.append(current.right)\n",
    "            else:\n",
    "                if len(queue) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"\\nNext Level\")\n",
    "                    queue.append(self.flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#This is the function which will calculate the error from our actual data labels and the predicted labels.\n",
    "\n",
    "def error(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a decision tree and finding the training error on it.\n",
    "# Parameter passed here is k.\n",
    "data_obj = GenerateDataset(4)\n",
    "# Parameters passed here are m and k respectively.\n",
    "x_train, y_train = data_obj.generate(30, 4)\n",
    "\n",
    "clf = DecisionTree()\n",
    "clf.train(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_train)\n",
    "myerror = error(y_train, y_pred)\n",
    "\n",
    "print(\"Error on training data is \", myerror)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of data and fit a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a decision tree and printing the nodes by level.\n",
    "\n",
    "data_obj = GenerateDataset(4)\n",
    "x_train, y_train = data_obj.generate(30, 4)\n",
    "\n",
    "clf = DecisionTree()\n",
    "clf.train(x_train, y_train)\n",
    "\n",
    "print(x_train, y_train)\n",
    "clf.print_tree(clf.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the typical (Average) error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Function to calculate averge error\n",
    "def average_error(y):\n",
    "    total, average = 0, 0\n",
    "    for i in y:\n",
    "        total += i\n",
    "    average = total / len(y)\n",
    "    return average * 100\n",
    "\n",
    "# As we have fit a decision tree in the previous question, we would just use that tree to test our data now.\n",
    "y = []\n",
    "i = 0\n",
    "while i <= 200:\n",
    "    \n",
    "    new_obj = GenerateDataset(4)\n",
    "    x_test, y_test = new_obj.generate(2000, 4)\n",
    "    \n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = error(y_test, y_pred)\n",
    "    y.append(acc)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"Some original labels: {}, Corresponding Predictions: {}\".format(y_test[:10], y_pred[:10]))\n",
    "print(\"Average Error = {}%\".format(average_error(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For k = 10, finding error and making graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def graph(x, y):\n",
    "    plt.plot(x, y, label=\"ID3 \", color=\"red\")\n",
    "    plt.xlabel(\"m values\")\n",
    "    plt.ylabel(\" $|err_{test} - err_{train}|$\")\n",
    "    plt.title(\"ID3\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for i in range(100, 10000, 250):\n",
    "    data_obj = GenerateDataset(10)\n",
    "    x_dataset, y_dataset = data_obj.generate(i, 10)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "    clf = DecisionTree()\n",
    "    clf.train(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = error(y_test, y_pred)\n",
    "\n",
    "    y_train_pred = clf.predict(x_train)\n",
    "    acc1 = error(y_train, y_train_pred)\n",
    "\n",
    "    y.append(abs(acc - acc1))\n",
    "    x.append(i)\n",
    "\n",
    "graph(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Alternative Metric - Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Node_gini:\n",
    "    def __init__(self, feature=None, value=None, *, left=None, right=None, label_name=None, entropy=None, samples=None,\n",
    "                 counts=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label_name = label_name\n",
    "        self.entropy = entropy\n",
    "        self.samples = samples\n",
    "        self.counts = counts\n",
    "\n",
    "\n",
    "class DecisionTree_gini:\n",
    "    flag = \"a\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def train(self, x_array, y):\n",
    "        _, features = x_array.shape\n",
    "        indices = [i for i in range(features)]\n",
    "        self.root = self.tree(x_array, y, indices)\n",
    "\n",
    "    def tree(self, x_array, y, feature_indices):\n",
    "        left_indices, right_indices = [], []\n",
    "        len_labels = len(set(y))\n",
    "        if len_labels == 1:\n",
    "            label_name = y[0]\n",
    "            return Node_gini(label_name=label_name)\n",
    "\n",
    "        if len_labels != 1 and len(feature_indices) == 0:\n",
    "            value = self.max_counter(y)\n",
    "            return Node_gini(label_name=value)\n",
    "\n",
    "        indexes = feature_indices\n",
    "        feature_index, entropy, feature_value = self.get_split_index(x_array, y, indexes, np.unique(y))\n",
    "        if entropy == 0:\n",
    "            value = self.max_counter(y)\n",
    "            return Node_gini(label_name=value)\n",
    "\n",
    "        x_column = x_array[:, feature_index]\n",
    "        len_x_column = len(x_column)\n",
    "        for i in range(len_x_column):\n",
    "            if x_column[i] == feature_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        left_node = self.tree(x_array[np.array(left_indices), :], y[np.array(left_indices)], indexes)\n",
    "        right_node = self.tree(x_array[np.array(right_indices), :], y[np.array(right_indices)], indexes)\n",
    "        return Node_gini(feature_index, feature_value, left=left_node, right=right_node, entropy=entropy,\n",
    "                    samples=len(y), counts=np.bincount(y))\n",
    "\n",
    "    def max_counter(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length = len(counts)\n",
    "        max_count, max_value = -1, -1\n",
    "        for i in range(length):\n",
    "            if counts[i] > max_count:\n",
    "                max_value = i\n",
    "        return max_value\n",
    "\n",
    "    def get_split_index(self, x_array, y, indexes, unique_y):\n",
    "        best_gain, split_index, split_value = 0, None, None\n",
    "        for column in indexes:\n",
    "            X_column = x_array[:, column]\n",
    "            left, right = np.array([]), np.array([])\n",
    "            for value in unique_y:\n",
    "                for i in range(len(X_column)):\n",
    "                    if X_column[i] == value:\n",
    "                        np.append(left, i)\n",
    "                    else:\n",
    "                        np.append(right, i)\n",
    "\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self.information_gain(left, right)\n",
    "                if gain < best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = column\n",
    "                    split_value = value\n",
    "\n",
    "        return split_index, best_gain, split_value\n",
    "    \n",
    "    # This is the information gain which we would get for gini index.\n",
    "    def information_gain(self, left, right):\n",
    "        probability = len(left) / (len(left) + len(right))\n",
    "        return (probability * self.gini(left)) - ((1-probability) * self.gini(right))\n",
    "    \n",
    "    # The gini index calculation is done here where we just pass the required row.\n",
    "    def gini(self, rows):\n",
    "        length = len(rows)\n",
    "        counts = np.bincount(rows)\n",
    "        impurity = 1\n",
    "        for i in range(len(counts)):\n",
    "            prob = counts[i] / length\n",
    "            impurity -= prob**2\n",
    "        return impurity\n",
    "\n",
    "    def predict(self, x_array):\n",
    "        return np.array([self.tree_traverse(x, self.root) for x in x_array])\n",
    "\n",
    "    def tree_traverse(self, x, node):\n",
    "        if node.label_name is not None:\n",
    "            return node.label_name\n",
    "\n",
    "        if x[node.feature] == node.value:\n",
    "            return self.tree_traverse(x, node.left)\n",
    "        return self.tree_traverse(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing gini on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gini_graph(x, y):\n",
    "    plt.plot(x, y, label=\"Gini\", color=\"red\")\n",
    "    plt.xlabel(\"m values\")\n",
    "    plt.ylabel(\" $|err_{test} - err_{train}|$\")\n",
    "    plt.title(\"Gini\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "gx, gy = [], []\n",
    "for i in range(100, 10000, 500):\n",
    "    data_obj = GenerateDataset(10)\n",
    "    gx_dataset, gy_dataset = data_obj.generate(i, 10)\n",
    "    gx_train, gx_test, gy_train, gy_test = train_test_split(gx_dataset, gy_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "    clf = DecisionTree_gini()\n",
    "    clf.train(gx_train, gy_train)\n",
    "\n",
    "    gy_pred = clf.predict(gx_test)\n",
    "    acc = error(gy_test, gy_pred)\n",
    "\n",
    "    gy_train_pred = clf.predict(gx_train)\n",
    "    acc1 = error(gy_train, gy_train_pred)\n",
    "\n",
    "    gy.append(abs(acc - acc1))\n",
    "    gx.append(i)\n",
    "\n",
    "gini_graph(gx, gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "plt.plot(gx, gy)\n",
    "plt.xlabel(\"Sample Size (m)\")\n",
    "plt.ylabel(\"|err_(train) - err|\")\n",
    "plt.legend([\"ID3\", \"Gini\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generating dataset by a scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GenerateDataset2:\n",
    "    def __init__(self):\n",
    "        self.x_array, self.y_array = [], []\n",
    "        \n",
    "    def generate_y_values(self, x_values):\n",
    "        if x_values[0] == 0:\n",
    "            y = Counter(x_values[1:6]).most_common()[0][0]\n",
    "        else:\n",
    "            y = Counter(x_values[6:11]).most_common()[0][0]\n",
    "        return y\n",
    "\n",
    "    def generate_data_points(self):\n",
    "        # Updated datapoints generation.\n",
    "        x_values = [choices([0, 1], [0.5, 0.5])[0]]\n",
    "\n",
    "        for i in range(1, 11):\n",
    "            x_values.append(choices([x_values[i - 1], (1 - x_values[i - 1])], [0.75, 0.25])[0])\n",
    "        for i in range(11, 16):\n",
    "            x_values.append(choices([1, 0], [0.5, 0.5])[0])\n",
    "        y_value = self.generate_y_values(x_values)\n",
    "        self.x_array.append(x_values)\n",
    "        self.y_array.append(y_value)\n",
    "        \n",
    "    def generate(self, m):\n",
    "        for i in range(m):\n",
    "            self.generate_data_points()\n",
    "        return np.array(self.x_array), np.array(self.y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Same function as the ID3. Added two functions at the end to find the irrelevant variables.\n",
    "class Node2:\n",
    "    def __init__(self, feature=None, value=None, *, left=None, right=None, label_name=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label_name = label_name\n",
    "\n",
    "\n",
    "class DecisionTree2:\n",
    "    flag = \"a\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        self.irrelevant_array, self.total_array = [], []\n",
    "        self.ir, self.total = 0, 0\n",
    "\n",
    "    def train(self, x_array, y):\n",
    "        _, features = x_array.shape\n",
    "        indices = [i for i in range(features)]\n",
    "        self.root = self.tree(x_array, y)\n",
    "\n",
    "    def tree(self, x_array, y):\n",
    "        left_indices, right_indices = [], []\n",
    "        len_labels = len(set(y))\n",
    "        if len_labels == 1:\n",
    "            label_name = y[0]\n",
    "            return Node2(label_name=label_name)\n",
    "\n",
    "        if len_labels == 1:\n",
    "            value = self.max_counter(y)\n",
    "            return Node2(label_name=value)\n",
    "\n",
    "        feature_index, entropy, feature_value = self.get_split_index(x_array, y, np.unique(y))\n",
    "        \n",
    "        self.total_array.append(feature_index)\n",
    "        self.total += 1\n",
    "        \n",
    "        # Counting the irrelevant variables in our tree.\n",
    "        if feature_index in [11, 12, 13, 14, 15]:\n",
    "            print(feature_index)\n",
    "            self.ir += 1\n",
    "            self.irrelevant_array.append(feature_index)\n",
    "                \n",
    "        if entropy == 0:\n",
    "            value = self.max_counter(y)\n",
    "            return Node2(label_name=value)\n",
    "\n",
    "        x_column = x_array[:, feature_index]\n",
    "        len_x_column = len(x_column)\n",
    "        for i in range(len_x_column):\n",
    "            if x_column[i] == feature_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        left_node = self.tree(x_array[np.array(left_indices), :], y[np.array(left_indices)])\n",
    "        right_node = self.tree(x_array[np.array(right_indices), :], y[np.array(right_indices)])\n",
    "        return Node2(feature_index, feature_value, left=left_node, right=right_node)\n",
    "\n",
    "    def max_counter(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length = len(counts)\n",
    "        max_count, max_value = -1, -1\n",
    "        for i in range(length):\n",
    "            if counts[i] > max_count:\n",
    "                max_value = i\n",
    "        return max_value\n",
    "\n",
    "    def get_split_index(self, x_array, y, unique_y):\n",
    "        entropy, split_index, split_value = -1, None, None\n",
    "        length = x_array.shape[1]\n",
    "        for column in range(length):\n",
    "            X_column = x_array[:, column]\n",
    "            for value in unique_y:\n",
    "                gain = self.information_gain(X_column, y, value)\n",
    "                if gain > entropy:\n",
    "                    entropy = gain\n",
    "                    split_index = column\n",
    "                    split_value = value\n",
    "\n",
    "        return split_index, entropy, split_value\n",
    "\n",
    "    def information_gain(self, X_column, y, split_value):\n",
    "        H_Y = self.H(y)\n",
    "\n",
    "        left_indices, right_indices = [], []\n",
    "        length = len(X_column)\n",
    "        for i in range(length):\n",
    "            if X_column[i] == split_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return 0\n",
    "\n",
    "        y_length = len(y)\n",
    "        H_Y_X = 0\n",
    "        x_values = {len(left_indices): y[left_indices], len(right_indices): y[right_indices]}\n",
    "        for value in x_values:\n",
    "            x_probability = value / y_length\n",
    "            H_Y_X += x_probability * self.H(x_values[value])\n",
    "\n",
    "        IG = H_Y - H_Y_X\n",
    "        return IG\n",
    "\n",
    "    def H(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length_y = len(y)\n",
    "        probabilities = np.array([i / length_y for i in counts])\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def predict(self, x_array):\n",
    "        return np.array([self.tree_traverse(x, self.root) for x in x_array])\n",
    "\n",
    "    def tree_traverse(self, x, node):\n",
    "        if node.label_name is not None:\n",
    "            return node.label_name\n",
    "\n",
    "        if x[node.feature] == node.value:\n",
    "            return self.tree_traverse(x, node.left)\n",
    "        return self.tree_traverse(x, node.right)\n",
    "\n",
    "    # The functions to find unique irrelevant variables and the probability of irrelevant variables respectively.\n",
    "    def check_irrelevant(self):\n",
    "        print(self.ir, self.total)\n",
    "    \n",
    "    def check_irrelevant_probability(self):\n",
    "        return self.ir\n",
    "    \n",
    "    def print_tree(self, root):\n",
    "        if root.feature is None and root.label_name is None:\n",
    "            return\n",
    "        \n",
    "        print(\"Level 0 (Root Node) \")\n",
    "        queue = [root, self.flag]\n",
    "\n",
    "        while True:\n",
    "            current = queue.pop(0)\n",
    "            if current != \"a\":\n",
    "                if current.label_name is not None:\n",
    "                    print(\"Value = {}\".format(current.label_name, end=\" \"))\n",
    "                else:\n",
    "                    print(\"Feature = {}\".format(current.feature, end=\" \"))\n",
    "                if current.left is not None:\n",
    "                    queue.append(current.left)\n",
    "                if current.right is not None:\n",
    "                    queue.append(current.right)\n",
    "            else:\n",
    "                if len(queue) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"\\nNext Level\")\n",
    "                    queue.append(self.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax, ay = [], []\n",
    "for i in range(50, 10000, 250):\n",
    "    absolute_error = 0\n",
    "    '''\n",
    "    We run iteration for a particular value of m for 5 times to find the error rate and average it to get the typical\n",
    "    error for a particular m. And finally, we plot the average error for a particular m.\n",
    "    '''\n",
    "    for k in range(0,5):\n",
    "        data_obj = GenerateDataset2()\n",
    "        ax_dataset, ay_dataset = data_obj.generate(i)\n",
    "        ax_train, ax_test, ay_train, ay_test = train_test_split(ax_dataset, ay_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "        clf = DecisionTree2()\n",
    "        clf.train(ax_train, ay_train)\n",
    "\n",
    "        ay_pred = clf.predict(ax_test)\n",
    "        acc = error(ay_test, ay_pred)\n",
    "\n",
    "        ay_train_pred = clf.predict(ax_train)\n",
    "        acc1 = error(ay_train, ay_train_pred)\n",
    "\n",
    "        absolute_error += abs(acc - acc1)\n",
    "        \n",
    "    # Finding average error for m.    \n",
    "    ay.append(absolute_error/5)\n",
    "    ax.append(i)\n",
    "\n",
    "plt.plot(ax,ay)\n",
    "plt.legend([\"ID3\"])\n",
    "plt.xlabel(\"Sample size (m)\")\n",
    "plt.ylabel(\"Average error rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irrelevant variables included in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probability, count = [], []\n",
    "it = [j for j in range(10, 50000, 1000)]\n",
    "for i in range(10, 50000, 1000):\n",
    "    count_prob, count_num, count_total = 0, 0, 0\n",
    "    for k in range(5):\n",
    "        data_obj = GenerateDataset2()\n",
    "        x_dataset, y_dataset = data_obj.generate(i)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "        clf = DecisionTree2()\n",
    "        clf.train(x_train, y_train)\n",
    "                  \n",
    "        irr_num = clf.check_irrelevant()\n",
    "        count_num += irr_num\n",
    "        ip = clf.check_irrelevant_probability()\n",
    "        count_prob += ip\n",
    "    count.append(count_num/5)\n",
    "    probability.append(count_prob/5)\n",
    "                  \n",
    "print(\"Average number of irrelevant variables in the decision tree: {}\".format(sum(count)/len(count)))\n",
    "print(\"Average probability of including an irrelevant variable in the decision tree: {}\".format(sum(probability)/len(probability)))\n",
    "\n",
    "plt.plot(it,probability)\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Random Variables Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pruning by Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NodeDepth:\n",
    "    def __init__(self, feature=None, value=None, *, left=None, right=None, label_name=None, entropy=None, samples=None,\n",
    "                 counts=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label_name = label_name\n",
    "        self.entropy = entropy\n",
    "        self.samples = samples\n",
    "        self.counts = counts\n",
    "\n",
    "class DecisionTreeDepth:\n",
    "    flag = \"a\"\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.irrelevant_array, self.relevant_array = [], []\n",
    "        self.ir, self.total = 0, 0\n",
    "\n",
    "    def train(self, x_array, y):\n",
    "        _, features = x_array.shape\n",
    "        indices = [i for i in range(features)]\n",
    "        self.root = self.tree(x_array, y)\n",
    "\n",
    "    '''\n",
    "    Here, I have passed a depth parameter which would be increased for each run. If the current depth equals this value,\n",
    "    then we give the value my majority vote.\n",
    "    '''\n",
    "    def tree(self, x_array, y, depth = 0):\n",
    "        left_indices, right_indices = [], []\n",
    "        len_labels = len(set(y))\n",
    "        if len_labels == 1:\n",
    "            label_name = y[0]\n",
    "            return NodeDepth(label_name=label_name)\n",
    "        \n",
    "        # Checking here if the current depth of the tree is equal to the parameter passed.\n",
    "        if len_labels == 1 or depth >= self.max_depth:\n",
    "            value = self.max_counter(y)\n",
    "            return NodeDepth(label_name = value)\n",
    "\n",
    "        feature_index, entropy, feature_value = self.get_split_index(x_array, y, np.unique(y))\n",
    "        \n",
    "        self.relevant_array.append(feature_index)\n",
    "        self.total += 1\n",
    "        \n",
    "        if feature_index in [11, 12, 13, 14, 15]:\n",
    "            self.ir += 1\n",
    "            self.irrelevant_array.append(feature_index)\n",
    "            \n",
    "        if entropy == 0:\n",
    "            value = self.max_counter(y)\n",
    "            return NodeDepth(label_name=value)\n",
    "\n",
    "        x_column = x_array[:, feature_index]\n",
    "        len_x_column = len(x_column)\n",
    "        for i in range(len_x_column):\n",
    "            if x_column[i] == feature_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        left_node = self.tree(x_array[np.array(left_indices), :], y[np.array(left_indices)], depth+1)\n",
    "        right_node = self.tree(x_array[np.array(right_indices), :], y[np.array(right_indices)], depth+1)\n",
    "        return NodeDepth(feature_index, feature_value, left=left_node, right=right_node, entropy=entropy,\n",
    "                    samples=len(y), counts=np.bincount(y))\n",
    "\n",
    "    def max_counter(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length = len(counts)\n",
    "        max_count, max_value = -1, -1\n",
    "        for i in range(length):\n",
    "            if counts[i] > max_count:\n",
    "                max_value = i\n",
    "        return max_value\n",
    "\n",
    "    def get_split_index(self, x_array, y, unique_y):\n",
    "        entropy, split_index, split_value = -1, None, None\n",
    "        length = x_array.shape[1]\n",
    "        for column in range(length):\n",
    "            X_column = x_array[:, column]\n",
    "            for value in unique_y:\n",
    "                gain = self.information_gain(X_column, y, value)\n",
    "                if gain > entropy:\n",
    "                    entropy = gain\n",
    "                    split_index = column\n",
    "                    split_value = value\n",
    "\n",
    "        return split_index, entropy, split_value\n",
    "\n",
    "    def information_gain(self, X_column, y, split_value):\n",
    "        H_Y = self.H(y)\n",
    "\n",
    "        left_indices, right_indices = [], []\n",
    "        length = len(X_column)\n",
    "        for i in range(length):\n",
    "            if X_column[i] == split_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return 0\n",
    "\n",
    "        y_length = len(y)\n",
    "        H_Y_X = 0\n",
    "        x_values = {len(left_indices): y[left_indices], len(right_indices): y[right_indices]}\n",
    "        for value in x_values:\n",
    "            x_probability = value / y_length\n",
    "            H_Y_X += x_probability * self.H(x_values[value])\n",
    "\n",
    "        IG = H_Y - H_Y_X\n",
    "        return IG\n",
    "\n",
    "    def H(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length_y = len(y)\n",
    "        probabilities = np.array([i / length_y for i in counts])\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def predict(self, x_array):\n",
    "        return np.array([self.tree_traverse(x, self.root) for x in x_array])\n",
    "\n",
    "    def tree_traverse(self, x, node):\n",
    "        if node.label_name is not None:\n",
    "            return node.label_name\n",
    "\n",
    "        if x[node.feature] == node.value:\n",
    "            return self.tree_traverse(x, node.left)\n",
    "        return self.tree_traverse(x, node.right)\n",
    "\n",
    "    def check_irrelevant(self):\n",
    "        return len(set(self.irrelevant_array))\n",
    "    \n",
    "    def check_irrelevant_probability(self):\n",
    "        return self.ir/self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, y, z = [], [], []\n",
    "it = [i for i in range(0, 15)]\n",
    "for i in range(0, 15):\n",
    "    data_obj = GenerateDataset2()\n",
    "    x_dataset, y_dataset = data_obj.generate(10000)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "    clf = DecisionTreeDepth(i)\n",
    "    clf.train(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = 100*error(y_test, y_pred)\n",
    "\n",
    "    y_train_pred = clf.predict(x_train)\n",
    "    acc1 = 100*error(y_train, y_train_pred)\n",
    "\n",
    "    y.append(acc)\n",
    "    z.append(acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(it,y)\n",
    "plt.plot(it, z)\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Error in %\")\n",
    "plt.legend([\"Testing\", \"Training\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning by Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NodeSample:\n",
    "    def __init__(self, feature=None, value=None, *, left=None, right=None, label_name=None, entropy=None, samples=None,\n",
    "                 counts=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label_name = label_name\n",
    "        self.entropy = entropy\n",
    "        self.samples = samples\n",
    "        self.counts = counts\n",
    "\n",
    "\n",
    "class DecisionTreeSample:\n",
    "    flag = \"a\"\n",
    "\n",
    "    def __init__(self, min_sample):\n",
    "        self.root = None\n",
    "        self.min_sample = min_sample\n",
    "        self.irrelevant_array, self.relevant_array = [], []\n",
    "        self.ir, self.total = 0, 0\n",
    "\n",
    "    def train(self, x_array, y):\n",
    "        _, features = x_array.shape\n",
    "        indices = [i for i in range(features)]\n",
    "        self.root = self.tree(x_array, y)\n",
    "\n",
    "    def tree(self, x_array, y):\n",
    "        left_indices, right_indices = [], []\n",
    "        len_labels = len(set(y))\n",
    "        \n",
    "        if len_labels == 1 or len(y) <= self.min_sample:\n",
    "            value = self.max_counter(y)\n",
    "            return NodeSample(label_name = value)\n",
    "\n",
    "        feature_index, entropy, feature_value = self.get_split_index(x_array, y, np.unique(y))\n",
    "        \n",
    "        self.relevant_array.append(feature_index)\n",
    "        self.total += 1\n",
    "        \n",
    "        if feature_index in [11, 12, 13, 14, 15]:\n",
    "            self.ir += 1\n",
    "            self.irrelevant_array.append(feature_index)\n",
    "            \n",
    "        if entropy == 0:\n",
    "            value = self.max_counter(y)\n",
    "            return NodeSample(label_name=value)\n",
    "\n",
    "        x_column = x_array[:, feature_index]\n",
    "        len_x_column = len(x_column)\n",
    "        for i in range(len_x_column):\n",
    "            if x_column[i] == feature_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        left_node = self.tree(x_array[np.array(left_indices), :], y[np.array(left_indices)])\n",
    "        right_node = self.tree(x_array[np.array(right_indices), :], y[np.array(right_indices)])\n",
    "        return NodeSample(feature_index, feature_value, left=left_node, right=right_node, entropy=entropy,\n",
    "                    samples=len(y), counts=np.bincount(y))\n",
    "\n",
    "    def max_counter(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length = len(counts)\n",
    "        max_count, max_value = -1, -1\n",
    "        for i in range(length):\n",
    "            if counts[i] > max_count:\n",
    "                max_value = i\n",
    "        return max_value\n",
    "\n",
    "    def get_split_index(self, x_array, y, unique_y):\n",
    "        entropy, split_index, split_value = -1, None, None\n",
    "        length = x_array.shape[1]\n",
    "        for column in range(length):\n",
    "            X_column = x_array[:, column]\n",
    "            for value in unique_y:\n",
    "                gain = self.information_gain(X_column, y, value)\n",
    "                if gain > entropy:\n",
    "                    entropy = gain\n",
    "                    split_index = column\n",
    "                    split_value = value\n",
    "\n",
    "        return split_index, entropy, split_value\n",
    "\n",
    "    def information_gain(self, X_column, y, split_value):\n",
    "        H_Y = self.H(y)\n",
    "\n",
    "        left_indices, right_indices = [], []\n",
    "        length = len(X_column)\n",
    "        for i in range(length):\n",
    "            if X_column[i] == split_value:\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return 0\n",
    "\n",
    "        y_length = len(y)\n",
    "        H_Y_X = 0\n",
    "        x_values = {len(left_indices): y[left_indices], len(right_indices): y[right_indices]}\n",
    "        for value in x_values:\n",
    "            x_probability = value / y_length\n",
    "            H_Y_X += x_probability * self.H(x_values[value])\n",
    "\n",
    "        IG = H_Y - H_Y_X\n",
    "        return IG\n",
    "\n",
    "    def H(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        length_y = len(y)\n",
    "        probabilities = np.array([i / length_y for i in counts])\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def predict(self, x_array):\n",
    "        return np.array([self.tree_traverse(x, self.root) for x in x_array])\n",
    "\n",
    "    def tree_traverse(self, x, node):\n",
    "        if node.label_name is not None:\n",
    "            return node.label_name\n",
    "\n",
    "        if x[node.feature] == node.value:\n",
    "            return self.tree_traverse(x, node.left)\n",
    "        return self.tree_traverse(x, node.right)\n",
    "\n",
    "    def check_irrelevant(self):\n",
    "        return len(set(self.irrelevant_array))\n",
    "    \n",
    "    def check_irrelevant_probability(self):\n",
    "        if self.total == 0:\n",
    "            return 0\n",
    "        return self.ir/self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, y, z = [], [], []\n",
    "it = [i for i in range(0, 10000, 250)]\n",
    "for i in range(0, 10000, 250):\n",
    "    data_obj = GenerateDataset2()\n",
    "    x_dataset, y_dataset = data_obj.generate(10000)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "    clf = DecisionTreeSample(i)\n",
    "    clf.train(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "    acc = 100*error(y_test, y_pred)\n",
    "\n",
    "    y_train_pred = clf.predict(x_train)\n",
    "    acc1 = 100*error(y_train, y_train_pred)\n",
    "\n",
    "    y.append(acc)\n",
    "    z.append(acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(it,y)\n",
    "plt.plot(it, z)\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.legend([\"Testing\", \"Training\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y, count = [], 0\n",
    "it = [j for j in range(10, 100000, 1000)]\n",
    "for i in range(10, 100000, 1000):\n",
    "    count = 0\n",
    "    for k in range(10):\n",
    "        data_obj = GenerateDataset2()\n",
    "        x_dataset, y_dataset = data_obj.generate(i)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "        clf = DecisionTree2()\n",
    "        clf.train(x_train, y_train)\n",
    "\n",
    "        a = clf.check_irrelevant()\n",
    "        count += a\n",
    "\n",
    "    y.append(count/10)\n",
    "\n",
    "plt.plot(it,y)\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Random Variables\")\n",
    "plt.show()\n",
    "\n",
    "print(count/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Finding spurious data after pruning sample by depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probability, count = [], []\n",
    "it = [j for j in range(10, 50000, 1000)]\n",
    "for i in range(10, 50000, 1000):\n",
    "    count_prob, count_num = 0, 0\n",
    "    for k in range(5):\n",
    "        data_obj = GenerateDataset2()\n",
    "        x_dataset, y_dataset = data_obj.generate(i)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "        clf = DecisionTreeDepth(8)\n",
    "        clf.train(x_train, y_train)\n",
    "\n",
    "        irr_num = clf.check_irrelevant()\n",
    "        count_num += irr_num\n",
    "        ip = clf.check_irrelevant_probability()\n",
    "        count_prob += ip\n",
    "    count.append(count_num/5)\n",
    "    probability.append(count_prob/5)\n",
    "                  \n",
    "print(\"Average number of irrelevant variables in the decision tree: {}\".format(sum(count)/len(count)))\n",
    "print(\"Average probability of including an irrelevant variable in the decision tree: {}\".format(sum(probability)/len(probability)))\n",
    "\n",
    "plt.plot(it,probability)\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Random Variables Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Finding spurious data after pruning sample by sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probability, count = [], []\n",
    "it = [j for j in range(10, 50000, 1000)]\n",
    "for i in range(10, 50000, 1000):\n",
    "    count_prob, count_num = 0, 0\n",
    "    for k in range(5):\n",
    "        data_obj = GenerateDataset2()\n",
    "        x_dataset, y_dataset = data_obj.generate(i)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=1234)\n",
    "\n",
    "        clf = DecisionTreeSample(1000)\n",
    "        clf.train(x_train, y_train)\n",
    "\n",
    "        irr_num = clf.check_irrelevant()\n",
    "        count_num += irr_num\n",
    "        ip = clf.check_irrelevant_probability()\n",
    "        count_prob += ip\n",
    "    count.append(count_num/5)\n",
    "    probability.append(count_prob/5)\n",
    "                  \n",
    "print(\"Average number of irrelevant variables in the decision tree: {}\".format(sum(count)/len(count)))\n",
    "print(\"Average probability of including an irrelevant variable in the decision tree: {}\".format(sum(probability)/len(probability)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(it,probability)\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Random Variables Probability\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}